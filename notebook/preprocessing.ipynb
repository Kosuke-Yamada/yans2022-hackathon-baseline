{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_file\", type=str, required=True)\n",
    "parser.add_argument(\"--output_dir\", type=str, required=True)\n",
    "parser.add_argument(\"--n_train\", type=int)\n",
    "parser.add_argument(\"--n_val\", type=int, default=100)\n",
    "parser.add_argument(\"--random_state\", type=int, default=0)\n",
    "\n",
    "args_list = [\"--input_file\", \"../data/dataset_shared_initial/training.jsonl\", \\\n",
    "            \"--output_dir\", \"../data/preprocessing_shared/\", \\\n",
    "            \"--n_train\", \"10\", \\\n",
    "            \"--n_val\", \"10\"]\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(args.input_file, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_sets(df, n_train, n_val, random_state):\n",
    "    df_product = df.groupby(\"product_idx\").count()\n",
    "    product_idx_list = sorted(set(df_product.index))\n",
    "\n",
    "    random.seed(random_state)\n",
    "    random.shuffle(product_idx_list)\n",
    "\n",
    "    val_list = product_idx_list[:n_val]\n",
    "    train_list = (\n",
    "        product_idx_list[n_val:]\n",
    "        if n_train is None\n",
    "        else product_idx_list[n_val : n_val + n_train]\n",
    "    )\n",
    "\n",
    "    sets_mapping = {}\n",
    "    sets_mapping.update({i: \"training-train\" for i in train_list})\n",
    "    sets_mapping.update({i: \"training-val\" for i in val_list})\n",
    "    df[\"sets\"] = df[\"product_idx\"].map(sets_mapping)\n",
    "    df[\"sets\"] = df[\"sets\"].fillna(\"disuse\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sets = decide_sets(df, args.n_train, args.n_val, args.random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sets.to_json(\n",
    "    args.output_dir + \"training.jsonl\",\n",
    "    orient=\"records\",\n",
    "    force_ascii=False,\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "df_tr = df_sets[df_sets[\"sets\"].str.contains(\"-train\")]\n",
    "df_tr.to_json(\n",
    "    args.output_dir + \"training-train.jsonl\",\n",
    "    orient=\"records\",\n",
    "    force_ascii=False,\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "df_val = df_sets[df_sets[\"sets\"].str.contains(\"-val\")]\n",
    "df_val.to_json(\n",
    "    args.output_dir + \"training-val.jsonl\",\n",
    "    orient=\"records\",\n",
    "    force_ascii=False,\n",
    "    lines=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hackathon_yans2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "048ea2002076220746db16da1077e6dfc79287068a1dc687e8059c61470fa061"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
